# Data Science Agent ğŸ¤–

A production-grade **autonomous AI agent** for end-to-end data science workflows. Upload datasets, describe your goal in natural language, and let the AI handle profiling, cleaning, feature engineering, model training, and visualization.

**Key Differentiator**: Not just a chatbot - a true AI agent with 75+ specialized tools, intelligent orchestration, dual LLM support, session memory, code interpreter, and Cloud Run API.

---

> ## ğŸ‰ **NEW: Modern React Frontend!**
> 
> The application now features a **professional React-based web interface** with a beautiful landing page and chat UI, replacing the old Gradio interface.
> 
> **Quick Start:**
> ```powershell
> .\start.ps1  # Windows
> ```
> or
> ```bash
> ./start.sh   # Linux/Mac
> ```
> 
> ğŸ“– **[See Full Frontend Integration Guide â†’](FRONTEND_INTEGRATION.md)**

---

## ğŸ¯ Project Vision

Build an **autonomous data science system** that achieves **50-70th percentile performance** on Kaggle competitions through intelligent automation, proving AI agents can handle real-world ML workflows end-to-end.

---

## âœ¨ Core Features

### **ğŸ¤– Intelligent Agent System**
- **82+ Specialized Tools** across 11 categories (profiling, cleaning, feature engineering, ML, visualization, BigQuery)
- **Dual LLM Support**: Groq (llama-3.3-70b) + Google Gemini (2.0-flash-exp)
- **Smart Orchestration**: LLM-powered function calling with intelligent tool chaining
- **Session Memory**: Contextual awareness across conversations ("cross-validate it", "try with Ridge")
- **Code Interpreter**: Write and execute custom Python code for tasks beyond predefined tools
- **Error Recovery**: Automatic retry with corrected parameters
- **Reasoning Modules**: Dedicated LLM reasoning layer with 19 specialized functions
- **Cloud Integration**: BigQuery data access + GCS artifact storage

### ğŸ¨ **Multiple Interfaces**
- **Gradio Web UI** (`chat_ui.py`): Upload files, chat interface, visual plots
- **CLI Interface** (`src/cli.py`): Command-line workflow automation
- **REST API** (`src/api/app.py`): Cloud Run-ready FastAPI wrapper
- **Python SDK**: Direct programmatic access

### ğŸ“Š **Complete ML Pipeline**
1. **Data Profiling** â†’ Statistics, types, quality issues
2. **Data Cleaning** â†’ Smart imputation, outlier handling, type conversion
3. **Feature Engineering** â†’ Time features, encoding, interactions, ratios
4. **Model Training** â†’ XGBoost, LightGBM, CatBoost, ensemble methods
5. **Hyperparameter Tuning** â†’ Optuna-based optimization
6. **Visualization** â†’ Matplotlib, Plotly, interactive dashboards
7. **EDA Reports** â†’ Sweetviz, ydata-profiling HTML reports
8. **Explainability** â†’ SHAP values, feature importance

### âš¡ **Performance & Scale**
- **Token Optimization**: 34% reduction in LLM context (compressed tool schemas)
- **SQLite Caching**: Memoization of expensive operations with TTL
- **Polars & DuckDB**: 10-100x faster than pandas for large datasets
- **Rate Limiting**: Intelligent API call management (Groq: 12K TPM, Gemini: 10 RPM)
- **Cloud Ready**: FastAPI service for Google Cloud Run deployment

---

## ğŸ—ï¸ Architecture

### **System Design**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Interfaces                            â”‚
â”‚  Gradio UI  â”‚  CLI  â”‚  REST API  â”‚  Python SDK               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DataScienceCopilot Orchestrator                 â”‚
â”‚  â€¢ LLM Function Calling (Groq/Gemini)                       â”‚
â”‚  â€¢ Session Memory Management                                 â”‚
â”‚  â€¢ Tool Execution & Chaining                                 â”‚
â”‚  â€¢ Error Recovery & Retry Logic                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    75+ Specialized Tools                     â”‚
â”‚  Data Profiling â”‚ Cleaning â”‚ Feature Engineering             â”‚
â”‚  Model Training â”‚ Visualization â”‚ EDA Reports                â”‚
â”‚  NLP/Text â”‚ Computer Vision â”‚ Time Series â”‚ MLOps           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Execution & Storage Backends                    â”‚
â”‚  Local: Polars, sklearn, XGBoost                            â”‚
â”‚  Cloud: BigQuery, Vertex AI, Cloud Storage (planned)        â”‚
â”‚  Cache: SQLite with TTL                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Tech Stack**

| Layer | Technologies |
|-------|-------------|
| **LLM** | Groq (llama-3.3-70b), Google Gemini (2.0-flash-exp) |
| **Data Processing** | Polars, DuckDB, Pandas, PyArrow, BigQuery |
| **ML/AI** | scikit-learn, XGBoost, LightGBM, CatBoost, Optuna |
| **Visualization** | Matplotlib, Seaborn, Plotly |
| **EDA Reports** | Sweetviz, ydata-profiling |
| **Explainability** | SHAP, LIME |
| **APIs** | FastAPI, Uvicorn |
| **UI** | Gradio, Typer + Rich (CLI) |
| **Storage** | SQLite (cache), CSV, Parquet, Google Cloud Storage |
| **Cloud** | Google Cloud Run, BigQuery, GCS, Vertex AI (planned) |

---

## ğŸš€ Quick Start

### **Prerequisites**
- Python 3.9+
- API Keys: [Groq](https://console.groq.com) or [Google AI Studio](https://makersuite.google.com/app/apikey)

### **Installation**

```bash
# Clone repository
git clone https://github.com/Surfing-Ninja/Data-Science-Agent.git
cd Data-Science-Agent

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env and add your API keys:
# GROQ_API_KEY=your_groq_key
# GOOGLE_API_KEY=your_google_key (optional)
# LLM_PROVIDER=groq  # or "gemini"
```

### **Usage Examples**

#### **1. Gradio Web UI** (Recommended for beginners)
```bash
python chat_ui.py
# Opens at http://localhost:7860
# Upload CSV â†’ Ask: "Analyze this data and predict house prices"
```

#### **2. CLI Interface**
```bash
# Complete workflow
python src/cli.py analyze data.csv --target price --task "Predict house prices"

# Quick profiling
python src/cli.py profile data.csv

# Train models only
python src/cli.py train cleaned.csv Survived --task-type classification
```

#### **3. Python SDK**
```python
from src.orchestrator import DataScienceCopilot

# Initialize agent
agent = DataScienceCopilot(
    provider="groq",  # or "gemini"
    reasoning_effort="medium"
)

# Run workflow
result = agent.analyze(
    file_path="titanic.csv",
    task_description="Build a model to predict passenger survival",
    target_col="Survived"
)

print(f"Status: {result['status']}")
print(f"Best Model: {result['best_model']}")
print(f"Accuracy: {result['best_score']}")
```

#### **4. REST API** (Cloud Run Ready)
```bash
# Start local server
cd src/api
python app.py
# Server runs at http://localhost:8080

# Make API call
curl -X POST http://localhost:8080/run \
  -F "file=@data.csv" \
  -F "task_description=Analyze and predict churn" \
  -F "target_col=churn"
```

---

## ğŸ“ Project Structure

```
Data-Science-Agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ orchestrator.py              # Main agent brain (1,136 lines)
â”‚   â”œâ”€â”€ cli.py                       # CLI interface (346 lines)
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ app.py                   # FastAPI Cloud Run wrapper (331 lines)
â”‚   â”œâ”€â”€ bigquery/                    # BigQuery integration ğŸ†•
â”‚   â”‚   â”œâ”€â”€ __init__.py             # BigQuery tools (4 functions)
â”‚   â”‚   â””â”€â”€ client.py               # BigQuery client wrapper
â”‚   â”œâ”€â”€ storage/                     # Artifact storage ğŸ†•
â”‚   â”‚   â”œâ”€â”€ artifact_store.py       # Local + GCS backends (613 lines)
â”‚   â”‚   â””â”€â”€ helpers.py              # Storage helper functions (125 lines)
â”‚   â”œâ”€â”€ reasoning/                   # LLM reasoning layer ğŸ†•
â”‚   â”‚   â”œâ”€â”€ __init__.py             # Core reasoning engine (350 lines)
â”‚   â”‚   â”œâ”€â”€ data_understanding.py   # Data insights (6 functions)
â”‚   â”‚   â”œâ”€â”€ model_explanation.py    # Model interpretation (6 functions)
â”‚   â”‚   â””â”€â”€ business_summary.py     # Business translations (7 functions)
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ cache_manager.py        # SQLite caching with TTL
â”‚   â”œâ”€â”€ tools/                       # 82+ specialized tools
â”‚   â”‚   â”œâ”€â”€ data_profiling.py       # Dataset analysis
â”‚   â”‚   â”œâ”€â”€ data_cleaning.py        # Cleaning & preprocessing
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py  # Feature creation
â”‚   â”‚   â”œâ”€â”€ model_training.py       # ML training
â”‚   â”‚   â”œâ”€â”€ visualization_engine.py # Matplotlib/Seaborn plots
â”‚   â”‚   â”œâ”€â”€ plotly_visualizations.py # Interactive charts
â”‚   â”‚   â”œâ”€â”€ eda_reports.py          # Sweetviz, ydata-profiling
â”‚   â”‚   â”œâ”€â”€ advanced_*.py           # Advanced features
â”‚   â”‚   â””â”€â”€ tools_registry.py       # All 82 tool definitions (1,600+ lines)
â”‚   â””â”€â”€ utils/                       # Helper utilities
â”‚       â”œâ”€â”€ polars_helpers.py       # Data manipulation
â”‚       â””â”€â”€ validation.py           # Input validation
â”œâ”€â”€ chat_ui.py                       # Gradio web interface (912 lines)
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ titanic_example.py           # Complete workflow demo
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ data/                        # Processed datasets
â”‚   â”œâ”€â”€ models/                      # Trained models (.pkl)
â”‚   â”œâ”€â”€ plots/                       # Visualizations (.png, .html)
â”‚   â””â”€â”€ reports/                     # EDA reports (.html)
â”œâ”€â”€ cache_db/                        # SQLite cache storage
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .env.example                     # Environment template
â””â”€â”€ README.md                        # This file
```

---

## ğŸ› ï¸ Tool Categories (82 Tools Total)

### **ğŸ“Š Data Profiling & Analysis (7 tools)**
- `profile_dataset`, `detect_data_quality_issues`, `analyze_correlations`, `get_smart_summary`, `compare_datasets`, `calculate_statistics`, `detect_skewness`

### **â˜ï¸ BigQuery Integration (4 tools)** ğŸ†•
- `bigquery_profile_table`, `bigquery_load_table`, `bigquery_execute_query`, `bigquery_write_results`

### **ğŸ§¹ Data Cleaning (8 tools)**
- `clean_missing_values`, `handle_outliers`, `remove_duplicates`, `filter_rows`, `rename_columns`, `drop_columns`, `sort_data`, `fix_data_types`

### **ğŸ”§ Feature Engineering (13 tools)**
- `encode_categorical`, `force_numeric_conversion`, `smart_type_inference`, `create_time_features`, `create_interaction_features`, `create_aggregation_features`, `create_ratio_features`, `create_statistical_features`, `create_log_features`, `create_binned_features`, `engineer_text_features`, `auto_feature_engineering`, `auto_feature_selection`

### **ğŸ¤– Model Training & Tuning (6 tools)**
- `train_baseline_models`, `hyperparameter_tuning`, `train_ensemble_models`, `perform_cross_validation`, `generate_model_report`, `auto_ml_pipeline`

### **ğŸ“ˆ Visualization (11 tools)**
- `generate_all_plots`, `generate_data_quality_plots`, `generate_eda_plots`, `generate_model_performance_plots`, `generate_feature_importance_plot`, `generate_interactive_scatter`, `generate_interactive_histogram`, `generate_interactive_correlation_heatmap`, `generate_interactive_box_plots`, `generate_interactive_time_series`, `generate_plotly_dashboard`

### **ğŸ“Š EDA Reports (3 tools)**
- `generate_sweetviz_report`, `generate_ydata_profiling_report`, `generate_combined_eda_report`

### **ğŸ”¬ Advanced Analysis (11 tools)**
- `perform_eda_analysis`, `detect_model_issues`, `detect_anomalies`, `detect_and_handle_multicollinearity`, `perform_statistical_tests`, `analyze_root_cause`, `detect_trends_and_seasonality`, `detect_anomalies_advanced`, `perform_hypothesis_testing`, `analyze_distribution`, `perform_segment_analysis`

### **ğŸ“ Data Wrangling (3 tools)**
- `merge_datasets`, `concat_datasets`, `reshape_dataset`

### **ğŸš€ MLOps & Production (5 tools)**
- `monitor_model_drift`, `explain_predictions`, `generate_model_card`, `perform_ab_test_analysis`, `detect_feature_leakage`

### **â° Time Series (3 tools)**
- `forecast_time_series`, `detect_seasonality_trends`, `create_time_series_features`

### **ğŸ’¼ Business Intelligence (4 tools)**
- `perform_cohort_analysis`, `perform_rfm_analysis`, `detect_causal_relationships`, `generate_business_insights`

### **ğŸ“š NLP/Text (4 tools)**
- `perform_topic_modeling`, `perform_named_entity_recognition`, `analyze_sentiment_advanced`, `perform_text_similarity`

### **ğŸ–¼ï¸ Computer Vision (3 tools)**
- `extract_image_features`, `perform_image_clustering`, `analyze_tabular_image_hybrid`

---

## ğŸ¯ Advanced Features

### **1. Session Memory**
The agent remembers context across conversations:

```python
# Conversation 1
"Train a model on earthquake.csv to predict magnitude"
â†’ Agent trains XGBoost, achieves 0.92 RÂ²

# Conversation 2 (Same session)
"Cross-validate it"
â†’ Agent knows: model=XGBoost, dataset=earthquake.csv, target=magnitude
â†’ Runs 5-fold CV automatically
```

### **2. Code Interpreter**
Execute custom Python code for tasks beyond predefined tools:

```python
User: "Make a Plotly scatter with custom dropdown filters"

Agent: execute_python_code(code='''
import plotly.graph_objects as go
df = pd.read_csv('./temp/data.csv')
# Custom visualization code...
fig.write_html('./outputs/code/custom_plot.html')
''')
```

### **3. Token Optimization**
System stays under LLM token limits even with 75 tools:

| Component | Before | After | Savings |
|-----------|--------|-------|---------|
| Tool Schemas | 8,193 tokens | 5,463 tokens | 34% |
| Tool Results | 5,000+ tokens | 50-200 tokens | 90%+ |

### **4. Error Recovery**
Agent learns from errors and auto-corrects:

```python
# Attempt 1
train_baseline_models(target_col="magnitude")
â†’ Error: Column 'magnitude' not found. Hint: Did you mean 'mag'?

# Attempt 2 (Automatic)
train_baseline_models(target_col="mag")
â†’ Success! Trained 4 models, best: XGBoost (0.92 RÂ²)
```

---

## â˜ï¸ Cloud Features

### **1. BigQuery Integration** ğŸ†•
Direct access to BigQuery tables without local downloads:

```python
# Profile a BigQuery table
agent.chat("Profile the table project.dataset.sales")

# Query and analyze
agent.chat("Query top 10 customers by revenue from BigQuery")

# Write results back
agent.chat("Write the cleaned data to BigQuery table project.dataset.sales_clean")
```

**Available Tools:**
- `bigquery_profile_table`: Get statistics for any BigQuery table
- `bigquery_load_table`: Load BigQuery data into local Polars DataFrame
- `bigquery_execute_query`: Run SQL queries directly on BigQuery
- `bigquery_write_results`: Write processed data back to BigQuery

**Setup:**
```bash
# Install BigQuery dependencies
pip install google-cloud-bigquery db-dtypes

# Set environment variable
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account-key.json"
```

**Looker-Compatible Schemas:**

The project defines stable BigQuery table schemas for BI tools (see [`BIGQUERY_SCHEMAS.md`](BIGQUERY_SCHEMAS.md)):
- ğŸ“Š `model_metrics` - Model performance tracking over time
- ğŸ¯ `feature_importance` - Feature impact analysis
- ğŸ”® `predictions` - Prediction monitoring with actuals
- ğŸ“‹ `data_profile_summary` - Data quality metrics

**Design Principles:**
- Stable schemas (no breaking changes without versioning)
- Consistent snake_case naming
- Clear dimension/metric separation
- Dashboard-ready with sample Looker views

### **2. Artifact Storage** ğŸ†•
Unified storage abstraction - switch between local and GCS with zero code changes:

```python
# Local storage (default)
agent.save_model(model, "my_model.pkl")  
# â†’ Saves to outputs/models/my_model.pkl

# GCS storage (automatic when GCS credentials present)
agent.save_model(model, "my_model.pkl")
# â†’ Saves to gs://your-bucket/models/my_model_v1.pkl with versioning
```

**Features:**
- **Automatic Backend Selection**: Uses GCS if credentials available, falls back to local
- **Versioning**: Automatic version suffixes for GCS artifacts
- **Metadata**: Stores creation time, size, checksums
- **Unified API**: Same code works for local and cloud storage

**Setup:**
```bash
# Install GCS dependencies
pip install google-cloud-storage

# Set bucket (optional, defaults to local)
export GCS_BUCKET="your-gcs-bucket-name"
```

### **3. Reasoning Modules** ğŸ†•
Dedicated LLM reasoning layer with clear boundaries (no raw data access, no training decisions):

```python
from reasoning.data_understanding import explain_dataset
from reasoning.model_explanation import explain_model_performance
from reasoning.business_summary import create_executive_summary

# Data insights
insights = explain_dataset(summary={
    "rows": 10000,
    "columns": 20,
    "missing_values": {"age": {"count": 150, "percentage": 1.5}}
})

# Model explanations
explanation = explain_model_performance(metrics={
    "accuracy": 0.95,
    "precision": 0.92,
    "recall": 0.88
}, task_type="classification")

# Business summaries
summary = create_executive_summary(
    project_results={"model_accuracy": 0.95},
    project_name="churn_prediction",
    business_objective="Reduce customer churn"
)
```

**19 Reasoning Functions:**
- **Data Understanding**: explain_dataset, suggest_transformations, identify_feature_engineering_opportunities, explain_missing_values, compare_datasets (6 functions)
- **Model Explanation**: explain_model_performance, interpret_feature_importance, diagnose_model_failure, explain_prediction, compare_models, explain_overfitting (6 functions)
- **Business Summary**: create_executive_summary, estimate_business_impact, create_stakeholder_report, translate_technical_to_business, prioritize_next_steps, explain_to_customer, assess_deployment_readiness (7 functions)

**Design Principles:**
- âœ… **NO Raw Data Access**: Only summaries/statistics allowed
- âœ… **NO Training Decisions**: Only explanations, never execution
- âœ… **Structured Output**: JSON schemas for cacheability
- âœ… **Dual Backend**: Works with both Gemini and Groq

---

## ğŸ”§ Configuration

### **Environment Variables** (`.env`)

```bash
# LLM Provider
LLM_PROVIDER=groq               # "groq" or "gemini"
GROQ_API_KEY=your_groq_key
GOOGLE_API_KEY=your_google_key  # Optional

# Model Selection
GROQ_MODEL=llama-3.3-70b-versatile
GEMINI_MODEL=gemini-2.0-flash-exp
REASONING_EFFORT=medium         # low, medium, high

# Cache Settings
CACHE_DB_PATH=./cache_db/cache.db
CACHE_TTL_SECONDS=86400         # 24 hours

# Cloud Features (Optional)
GCS_BUCKET=your-gcs-bucket-name                           # For artifact storage
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-key.json  # For BigQuery + GCS

# Cloud Run (for API deployment)
PORT=8080
```

### **Provider Comparison**

| Feature | Groq | Gemini |
|---------|------|--------|
| **Model** | llama-3.3-70b-versatile | gemini-2.0-flash-exp |
| **Speed** | âš¡ Extremely fast (LPU) | ğŸš€ Very fast |
| **Free Tier** | 100K tokens/day | 1,500 requests/day |
| **Rate Limit** | 12K tokens/min | 10 requests/min |
| **Best For** | High-volume, low-latency | Free tier, high quota |

---

## ğŸš€ Cloud Deployment (Google Cloud Run)

### **Deploy REST API**

```bash
# 1. Build Docker image (Dockerfile provided)
docker build -t data-science-agent .

# 2. Push to Google Container Registry
gcloud builds submit --tag gcr.io/PROJECT_ID/data-science-agent

# 3. Deploy to Cloud Run
gcloud run deploy data-science-agent \
  --image gcr.io/PROJECT_ID/data-science-agent \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 4Gi \
  --timeout 3600 \
  --set-env-vars GROQ_API_KEY=your_key,LLM_PROVIDER=groq

# 4. Test deployment
curl -X POST https://your-service-url/run \
  -F "file=@data.csv" \
  -F "task_description=Predict churn"
```

### **API Endpoints**

- `GET /` - Health check
- `GET /health` - Readiness probe
- `POST /run` - Full analysis workflow
- `POST /profile` - Quick dataset profiling
- `GET /tools` - List all available tools

---

## ğŸ—ºï¸ Roadmap

### **Phase 1: Core Agent** âœ… COMPLETE
- [x] 75 specialized tools
- [x] Dual LLM support (Groq + Gemini)
- [x] CLI + Gradio UI
- [x] SQLite caching
- [x] Token optimization

### **Phase 2: Intelligence** âœ… COMPLETE
- [x] Session memory
- [x] Code interpreter
- [x] Error recovery
- [x] EDA reports (Sweetviz, ydata-profiling)
- [x] Interactive Plotly visualizations

### **Phase 3: Cloud Native** âœ… COMPLETE
- [x] FastAPI Cloud Run wrapper with 4 REST endpoints
- [x] BigQuery integration (4 tools: profile, load, query, write)
- [x] Artifact Storage abstraction (Local â†” GCS switching)
- [x] Reasoning modules for LLM explanations (19 functions)
- [x] Looker-compatible BigQuery schemas (4 stable tables)
- [ ] Vertex AI model training (planned)
- [ ] Cloud Logging & Monitoring (planned)

### **Phase 4: Enterprise** ğŸ“‹ PLANNED
- [ ] Multi-user authentication
- [ ] Team workspaces
- [ ] Model registry
- [ ] Automated retraining pipelines

### **Phase 5: Kaggle Integration** ğŸ¯ FUTURE
- [ ] Direct Kaggle API integration
- [ ] Automated competition workflow
- [ ] Ensemble strategies
- [ ] Submission automation

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

1. **New Tools**: Time series forecasting, NLP preprocessing, image augmentation
2. **Cloud Backends**: AWS, Azure support
3. **Performance**: Optimize tool execution, reduce latency
4. **UI/UX**: Better visualization, workflow builder
5. **Documentation**: Tutorials, video guides, blog posts

---

## ğŸ“œ License

MIT License - See LICENSE file for details

---

## ğŸ“§ Support & Community

- **Issues**: [GitHub Issues](https://github.com/Surfing-Ninja/Data-Science-Agent/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Surfing-Ninja/Data-Science-Agent/discussions)

---

## ğŸ“Š Project Stats

- **Lines of Code**: ~18,000+
- **Tools**: 82 specialized functions (75 core + 4 BigQuery + 3 storage helpers)
- **Reasoning Functions**: 19 LLM-powered explanation modules
- **Supported Models**: 10+ (LR, Ridge, Lasso, RF, XGBoost, LightGBM, CatBoost, etc.)
- **Visualization Types**: 20+ (static + interactive)
- **Data Formats**: CSV, Parquet, JSON, BigQuery tables
- **Cloud Platforms**: Google Cloud (Run, BigQuery, GCS) - AWS/Azure planned

---

<div align="center">

**Built with â¤ï¸ for the Data Science Community**

*"Making data science accessible through AI automation"*

â­ Star this repo if you find it useful! â­

</div>
